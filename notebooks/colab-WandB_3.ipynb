{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pradhapmoorthi/FirstML/blob/main/notebooks/colab-WandB_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pVhOfzLx9us"
      },
      "source": [
        "# Using Google Colab with GitHub\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKJ4bd5rt1wy"
      },
      "source": [
        "\n",
        "[Google Colaboratory](http://colab.research.google.com) is designed to integrate cleanly with GitHub, allowing both loading notebooks from github and saving notebooks to github."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-NVg7RjyeTk"
      },
      "source": [
        "## Loading Public Notebooks Directly from GitHub\n",
        "\n",
        "Colab can load public github notebooks directly, with no required authorization step.\n",
        "\n",
        "For example, consider the notebook at this address: https://github.com/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb.\n",
        "\n",
        "The direct colab link to this notebook is: https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb.\n",
        "\n",
        "To generate such links in one click, you can use the [Open in Colab](https://chrome.google.com/webstore/detail/open-in-colab/iogfkhleblhcpcekbiedikdehleodpjo) Chrome extension."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jCH8h6MlkkjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzIRIt9d2huC"
      },
      "source": [
        "## Browsing GitHub Repositories from Colab\n",
        "\n",
        "Colab also supports special URLs that link directly to a GitHub browser for any user/organization, repository, or branch. For example:\n",
        "\n",
        "- http://colab.research.google.com/github will give you a general github browser, where you can search for any github organization or username.\n",
        "- http://colab.research.google.com/github/googlecolab/ will open the repository browser for the ``googlecolab`` organization. Replace ``googlecolab`` with any other github org or user to see their repositories.\n",
        "- http://colab.research.google.com/github/googlecolab/colabtools/ will let you browse the main branch of the ``colabtools`` repository within the ``googlecolab`` organization. Substitute any user/org and repository to see its contents.\n",
        "- http://colab.research.google.com/github/googlecolab/colabtools/blob/main will let you browse ``main`` branch of the ``colabtools`` repository within the ``googlecolab`` organization. (don't forget the ``blob`` here!) You can specify any valid branch for any valid repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rmai0dD30XzL"
      },
      "source": [
        "## Loading Private Notebooks\n",
        "\n",
        "Loading a notebook from a private GitHub repository is possible, but requires an additional step to allow Colab to access your files.\n",
        "Do the following:\n",
        "\n",
        "1. Navigate to http://colab.research.google.com/github.\n",
        "2. Click the \"Include Private Repos\" checkbox.\n",
        "3. In the popup window, sign-in to your Github account and authorize Colab to read the private files.\n",
        "4. Your private repositories and notebooks will now be available via the github navigation pane."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J3NBxtZpPcK"
      },
      "source": [
        "## Saving Notebooks To GitHub or Drive\n",
        "\n",
        "Any time you open a GitHub hosted notebook in Colab, it opens a new editable view of the notebook. You can run and modify the notebook without worrying about overwriting the source.\n",
        "\n",
        "If you would like to save your changes from within Colab, you can use the File menu to save the modified notebook either to Google Drive or back to GitHub. Choose **File→Save a copy in Drive** or **File→Save a copy to GitHub** and follow the resulting prompts. To save a Colab notebook to GitHub requires giving Colab permission to push the commit to your repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QAWNjizy_3O"
      },
      "source": [
        "## Open In Colab Badge\n",
        "\n",
        "Anybody can open a copy of any github-hosted notebook within Colab. To make it easier to give people access to live views of GitHub-hosted notebooks,\n",
        "colab provides a [shields.io](http://shields.io/)-style badge, which appears as follows:\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "\n",
        "The markdown for the above badge is the following:\n",
        "\n",
        "```markdown\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "```\n",
        "\n",
        "The HTML equivalent is:\n",
        "\n",
        "```HTML\n",
        "<a href=\"https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "```\n",
        "\n",
        "Remember to replace the notebook URL in this template with the notebook you want to link to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VQqVi-3ScBC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da6fb647"
      },
      "source": [
        "# Task\n",
        "Modify the existing `keras.Sequential` model by adding an additional `Dense` layer with 128 units and ReLU activation between the `Flatten` layer and the final output `Dense` layer, then recompile and retrain the modified model using the same optimizer, loss function, and metrics. After retraining, plot the training and validation accuracy over epochs, and finally, summarize the changes made to the model, the results of the retraining, and the insights gained from the accuracy plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3661edf"
      },
      "source": [
        "## Add Dense Layer to Model\n",
        "\n",
        "### Subtask:\n",
        "Modify the existing `keras.Sequential` model by inserting an additional `Dense` layer with 128 units and ReLU activation between the `Flatten` layer and the final output `Dense` layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "109f14dc"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying the existing Keras model to insert an additional Dense layer. This will be achieved by redefining the `model` variable with the specified `Flatten` layer, the new `Dense` layer, and the final output `Dense` layer, and then recompiling the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5b7291f"
      },
      "source": [
        "## Recompile and Retrain Model\n",
        "\n",
        "### Subtask:\n",
        "Recompile the modified model with the same optimizer ('adam'), loss function (`SparseCategoricalCrossentropy`), and metrics ('accuracy'). Then, retrain the model using the `train_dataset` and `eval_dataset` for the specified number of epochs (`EPOCHS`), capturing the training history.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4f59e7d"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires retraining the modified model and capturing its training history. This step will execute the `model.fit` method using the previously defined datasets and number of epochs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1074521"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to plot the training and validation accuracy over epochs using the `history` object obtained from the retraining process. This visualization will help in analyzing the model's performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a538d94b"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `plt` (matplotlib.pyplot) was not defined. Although `matplotlib.pyplot` was imported in an earlier cell (`iD-M-MBskkV5`), the execution state might have been reset or the cell was not run, leading to the `NameError`. To fix this, I need to explicitly import `matplotlib.pyplot as plt` in the current cell before using it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7cb72f5"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed with a `NameError` because the `history` variable, which stores the training results, was not defined in the current execution context. This likely happened because the preceding cell (`c28abe1a`) where `history` is generated by `model.fit()` was not executed or its state was lost. To ensure `history` is available for plotting, I will re-include the `model.fit()` call within this cell before the plotting code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "523b5403"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed with a `NameError` because `model` was not defined. This is because the cell defining and compiling the model (`88f6346e`) was not executed or its state was lost. To resolve this, I will re-include the model definition and compilation steps, along with the import and training/plotting code, in this single code block to ensure all necessary variables are initialized.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1d950fb"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because several constants (`IMG_HEIGHT`, `IMG_WIDTH`, `IMG_CHANNELS`, `CLASS_NAMES_LIST`, `EPOCHS`) and datasets (`train_dataset`, `eval_dataset`) were not defined in the current execution context. These were originally defined in an earlier cell that might not have been executed. To resolve this, I will re-include their definitions and the necessary data loading and preprocessing steps within this code block, along with the model definition, compilation, training, and plotting code, to ensure all dependencies are met.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb42da10"
      },
      "source": [
        "# Task\n",
        "Okay, I will proceed with executing the current cell."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X7Vl7OMqlYfE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53ed8692"
      },
      "source": [
        "## Reinitialize Environment and Define Model\n",
        "\n",
        "### Subtask:\n",
        "Re-import necessary libraries, redefine global constants, re-establish data loading and preprocessing pipelines, build tf.data.Datasets, and define and compile the keras.Sequential model with an added Dense layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33dcb3b8"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires reinitializing the environment by re-importing libraries, redefining global constants, re-establishing data loading and preprocessing pipelines, rebuilding tf.data.Datasets, and defining and compiling the Keras model. The provided code block from the previous step (`093df487`) already encompasses all these instructions, including the model definition with the added dense layer, compilation, training, and plotting, which is crucial for a complete re-initialization and execution after previous `NameError` issues.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import wandb\n",
        "from wandb.integration.keras import WandbMetricsLogger\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# Sweep configuration\n",
        "sweep_config = {\n",
        "    'method': 'grid',\n",
        "    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
        "    'parameters': {\n",
        "        'batch_size': {'values': [8, 16]},\n",
        "        'learning_rate': {'values': [0.001, 0.0001]},\n",
        "        'hidden_nodes': {'values': [128, 64]},\n",
        "        'img_size': {'values': [16, 224]},\n",
        "        'epochs': {'values': [5, 10]}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Create sweep\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"5-flowers-newer\")\n",
        "\n",
        "# Training function\n",
        "def train():\n",
        "    with wandb.init() as run:\n",
        "        config = wandb.config\n",
        "\n",
        "        IMG_HEIGHT = config.img_size\n",
        "        IMG_WIDTH = config.img_size\n",
        "        IMG_CHANNELS = 3\n",
        "\n",
        "        CLASS_NAMES_LIST = [\"daisy\", \"dandelion\", \"roses\", \"sunflowers\", \"tulips\"]\n",
        "        CLASS_NAMES_TF = tf.constant(CLASS_NAMES_LIST)\n",
        "\n",
        "        TRAIN_CSV = \"gs://cloud-ml-data/img/flower_photos/train_set.csv\"\n",
        "        EVAL_CSV = \"gs://cloud-ml-data/img/flower_photos/eval_set.csv\"\n",
        "\n",
        "        AUTOTUNE = tf.data.AUTOTUNE\n",
        "        BATCH_SIZE = config.batch_size\n",
        "        EPOCHS = config.epochs\n",
        "\n",
        "        # Image reading & parsing\n",
        "        def read_and_decode(filename, resize_dims):\n",
        "            img_bytes = tf.io.read_file(filename)\n",
        "            img = tf.image.decode_jpeg(img_bytes, channels=IMG_CHANNELS)\n",
        "            img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "            img = tf.image.resize(img, resize_dims)\n",
        "            return img\n",
        "\n",
        "        def parse_csvline(csv_line):\n",
        "            record_defaults = [\"\", \"\"]\n",
        "            filename, label_string = tf.io.decode_csv(csv_line, record_defaults)\n",
        "            img = read_and_decode(filename, [IMG_HEIGHT, IMG_WIDTH])\n",
        "            label = tf.argmax(tf.cast(tf.equal(CLASS_NAMES_TF, label_string), tf.int32), axis=0, output_type=tf.int32)\n",
        "            return img, label\n",
        "\n",
        "        # Build datasets\n",
        "        train_dataset = (\n",
        "            tf.data.TextLineDataset(TRAIN_CSV)\n",
        "              .map(parse_csvline, num_parallel_calls=AUTOTUNE)\n",
        "              .shuffle(1000, reshuffle_each_iteration=True)\n",
        "              .batch(BATCH_SIZE)\n",
        "              .prefetch(AUTOTUNE)\n",
        "        )\n",
        "\n",
        "        eval_dataset = (\n",
        "            tf.data.TextLineDataset(EVAL_CSV)\n",
        "              .map(parse_csvline, num_parallel_calls=AUTOTUNE)\n",
        "              .batch(BATCH_SIZE)\n",
        "              .prefetch(AUTOTUNE)\n",
        "        )\n",
        "\n",
        "        # Define model\n",
        "        model = keras.Sequential([\n",
        "            keras.layers.Flatten(input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)),\n",
        "            keras.layers.Dense(config.hidden_nodes, activation=\"relu\"),\n",
        "            keras.layers.Dense(len(CLASS_NAMES_LIST), activation=\"softmax\")\n",
        "        ])\n",
        "\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=config.learning_rate)\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                      metrics=[\"accuracy\"])\n",
        "\n",
        "        # Train model\n",
        "        model.fit(train_dataset,\n",
        "                  validation_data=eval_dataset,\n",
        "                  epochs=EPOCHS,\n",
        "                  callbacks=[WandbMetricsLogger()])\n",
        "\n",
        "# Run sweep agent\n",
        "wandb.agent(sweep_id, function=train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "5L-0oxOhiXw6",
        "outputId": "d3705dd0-bc22-47b3-9ee0-2413b7a15781"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2925282369.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Create sweep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0msweep_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msweep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msweep_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"5-flowers-newer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Training function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_sweep.py\u001b[0m in \u001b[0;36msweep\u001b[0;34m(sweep, entity, project, prior_runs)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# Make sure we are logged in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mwandb_login\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_silent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInternalApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0msweep_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsert_sweep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msweep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_runs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprior_runs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, verify, referrer, update_api_key, _silent, _disable_warning)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mkey_is_pre_configured\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferrer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreferrer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(self, referrer)\u001b[0m\n\u001b[1;32m    239\u001b[0m     ) -> tuple[str | None, ApiKeyStatus]:\n\u001b[1;32m    240\u001b[0m         \u001b[0;34m\"\"\"Updates the global API key by prompting the user.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferrer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mApiKeyStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTTY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             directive = (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_prompt_api_key\u001b[0;34m(self, referrer)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 key = apikey.prompt_api_key(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                     \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/lib/apikey.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, no_offline, no_create, local, referrer)\u001b[0m\n\u001b[1;32m    163\u001b[0m             )\n\u001b[1;32m    164\u001b[0m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtermlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_api_key_prompt_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferrer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_ask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhide\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLOGIN_CHOICE_NOTTY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;31m# TODO: Needs refactor as this needs to be handled by caller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/errors/term.py\u001b[0m in \u001b[0;36mterminput\u001b[0;34m(prompt, timeout, hide)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \"\"\"\n\u001b[1;32m    292\u001b[0m     \u001b[0mprefixed_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{LOG_STRING}: {prompt}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_terminput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefixed_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhide\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhide\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/errors/term.py\u001b[0m in \u001b[0;36m_terminput\u001b[0;34m(prefixed_prompt, timeout, hide)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mclick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAbort\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "colab-github-demo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}